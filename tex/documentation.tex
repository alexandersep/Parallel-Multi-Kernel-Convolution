%%
%% LaTeX Boilerplate (http://github.com/gbluma/latex-boilerplate/)
%%
\documentclass[12pt,fleqn,leqno,letterpaper]{article}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage[margin=0.6in]{geometry}
\usetikzlibrary{positioning,matrix, arrows.meta}

\include{preamble}

\hypersetup{
    linktoc=all,     %set to all if you want both sections and subsections linked
}

\title{CSU33014 Lab 2: Parallel multichannel multikernel convolution}
\author{Alexander Sepelenco 20335014, Niall Sauvage 20334203\\
  \small{Trinity College Dublin}
}
%\date{December 15, 2012}

\begin{document}

\maketitle
\tableofcontents

\newpage    
\section{How to run the code}
Our code was designed with Stoker in mind, in order to ensure the
same compilation, Our Makefile is attachted with our submission, 
use it in order to ensure one to one compilation.

\section{Parallel multichannel convolution}
Parallel multichannel convolution consists of 3d images, 4d kernels, that means there
are multiple 2d images, in our case "channels" worth of 2d images, a width and a hieght, 
and "channels" worth of 2d kernels, we also have another dimension so "nkernels" worth of 3d kernels. 
The goal of the assignment is to optimise convolutions in a way that utilizes parallelised code mainly.
We focused on parallelisation and overall speed, utilizing a variety of techniques.
These techniques are discussed in the subsections.

\subsection{Flatland}
One of our first key insights was how memory was allocated - instead of several non-contiguous arrays of
pointers, we realised that at the lowest level, all the data was contiguous. This allowed us to minimise 
memory accesses by using multiplication and addition to compute the offset of any element from the first 
element in the array. This is much faster in terms of cycles than memory accesses, but it led to more 
arithmetic being performed. To remedy this, we precalculated some values in our sum. For example, if we
entered the relevant section of the "m" loop, then we would calculate the offset given by the m value. 
This allowed us to minimise time spent waiting for memory accesses and minimise unneccessary arithmetic.

\subsection{Loop Fusing}
The discussion on loop fusing was inspired from searching for optimisation strategies online.
This stackoverflow discussion on loop fusing was beneficial in our programme, 
This example below is how loop fusing works, given nested arrays, they can mathematically be combined into 
one using division and modulus. In fact this seems slower, however due to the ability to parallelise more
effectively division is the least of our problems. In fact this has an openmp name, collapse(). However 
we decided to do it manually as it was our first solution, and it is clearer to the programmer rather than
having it behind a macro of openmp.

\subsection{Unrolling}
In order to increase cache hits, we attempted to unroll the inner (or tight) loop. Initially this was done 
with GCC using \lstinline{#pragma GCC unroll 16} as we knew that the number of channels was a multiple of 2 of minimum 32.
As we were doing two calculations per loop iteration, we could then do a loop of size 32 (2 * 16) and this would
be usable with any correct kernel. However, by inspecting the size of the executable, we noticed that the size did not change
with or without the directive, implying it was not working. Moreover, we discovered we could do 4 instructions per tight loop
as we could load 4 image values into a vector, cast the lowest two to doubles, multiply them and then shuffle our original vector
such that we could then repeat the process with the upper two lanes. We then decided to manually unroll the loop, as is shown in 
the final version of our code. This allowed us less branches and greater cache hit rate.

\newpage
\begin{verbatim}
for ( m = 0; m < nkernels; m++ ) {
    for ( h = 0; h < height; h++ ) {
        for ( w = 0; w < width; w++ ) {
            // code
        }
    }
}
\end{verbatim}

\begin{verbatim}
#pragma omp parallel for
for (int n = 0; n < (nkernels*width*height); n++) {
    int m = n/(width*height);
    int w = (n%(width*height))/height;
    int h = (n%(width*height))%height;
    // code
}
\end{verbatim}


\subsection{Vectorisation}
Effective vectorisation requires the arrays to be loaded in contiguous memory from an array. The
original section of the code however only has 1 array be in contiguous memory, since we would like to
guarantee better cache hits we moved the channels for loop to the most inner for loop, as the amount of
channels for each kernel image can be at a maximum 2048 which is quite high. 
Transposing the kernel array was a key factor. This allowed us to create an array that can be loaded in contiguous
memory from channels. Another issue that we faced was that the horizontal addition of 4 very big floats results in
an inaccuracy in our programme, a fix for that was that we decided to deal with double type vectors (\_\_m128d), this
ensured accuracy. Below is a section of parallelisation ustilizing our loop fusing and vectorised code section utilizing
SSE1-4. SSE part requires loading as float casting to double, then loading our double tranposed kernel, and multiplying
the two vectors against each other, adding them up for all the channels, and then once the kernel\_order on x an y of kernels
has been executed sum up the vector using a single hadd instruction. The reason is because \_\_m128d \_mm\_hadd\_pd(v4sum), only
stores two lanes, therefore 1 instruction is needed rather than the typical four we would see with float vectors (\_\_mm128).
\newpage
\begin{verbatim}
// Transpose Kernel
#pragma omp parallel for // maybe this will numb the pain
for(int n = 0; n < (nkernels*nchannels*kernel_order); n++){
    int m = n/(nchannels*kernel_order);
    int c = (n%(nchannels*kernel_order))/kernel_order; 
    int x = (n%(nchannels*kernel_order))%kernel_order; 
    m_times_kernel_offset = m * kernel_offset;
    c_times_ko2 = c * ko2;
    x_times_kernel_order = x * kernel_order;
    kernel_total_offset_precalc = m_times_kernel_offset + 
        x_times_kernel_order + c_times_ko2;
    t_kernel_total_offset_precalc = m_times_kernel_offset + 
        x_times_kernel_order * nchannels + c;
    for(int y = 0; y < kernel_order; y++){
        t_kernel[t_kernel_total_offset_precalc + y * nchannels] = 
            (double) kernel[kernel_total_offset_precalc + y];
    }
}

// code
// __m128d v4sum = _mm_setzero_pd();
// ... more for loops ...
// Vectorised Section
        for ( c = 0; c < nchannels; c+=2 ) {
            // t_kernel is aligned hence the load instead of loadu
            // cvtps convert float vector to double vector 4 -> 2 (lower) lanes
            // shuffle by moving 2 high values to take 2 low values spot
            // mul two double vectors, add them, and sum them up.
            __m128 v4image_1d = _mm_loadu_ps(image_1d+image_offset+c);
            __m128d v4image_1d_pd = _mm_cvtps_pd(v4image_1d);

            __m128d v4t_kernel_pd = _mm_load_pd(t_kernel+kernel_total_offset+c);

            __m128d product = _mm_mul_pd(v4image_1d_pd, v4t_kernel_pd);
            v4sum = _mm_add_pd(v4sum, product);

            c+=2;
            v4image_1d = _mm_shuffle_ps(v4image_1d, v4image_1d, _MM_SHUFFLE(0, 0, 3, 2));
            v4image_1d_pd = _mm_cvtps_pd(v4image_1d);

            v4t_kernel_pd = _mm_load_pd(t_kernel+kernel_total_offset+c);

            product = _mm_mul_pd(v4image_1d_pd, v4t_kernel_pd);
            v4sum = _mm_add_pd(v4sum, product);

            c+=2;
            //unrolled 8 times
        }
    }
}
v4sum = _mm_hadd_pd(v4sum, v4sum); // add the two lanes together and put in lower lane
output1d[m * width_times_height + w * height + h] = (float) _mm_cvtsd_f64(v4sum); // extract lower double 
// .. end of for loops
\end{verbatim}
Since nchannels is guaranteed to be a power of 2, a postloop is not required for this for loop.

\subsubsection{Alignment}
Alignmnet was used on our transposed array, as it is an array we can align by 16 bytes and allow
for the usage of \_mm\_load\_pd, in order to guarantee more cache hits.

\begin{verbatim}
void* _mm_malloc (size_t size, size_t align) // for aligning in intel architecutre
void _mm_free (void * mem_addr) // free aligned intel align call
__m128d _mm_load_pd(double const * mem_addr); // load 16 byte aligned memory location
\end{verbatim}

\section{Speed Tests}
Tested using ./conv 128 128 7 128 128 
\begin{verbatim}
David conv time: 24483093 microseconds
Student conv time: 602688 microseconds
The total speed up time was 40.62x and 23880405 microseconds less
COMMENT: sum of absolute differences (0.000000) within acceptable range (0.062500)

David conv time: 24629741 microseconds
Student conv time: 629424 microseconds
The total speed up time was 39.13x and 24000317 microseconds less
\end{verbatim}
The code from multiple tests, averages around 39 times the speed increase from the original
code.

\section{Sources}
\begin{verbatim}
https://stackoverflow.com/questions/18749493/
openmp-drastically-slows-down-for-loop/18763554#18763554}

https://stackoverflow.com/questions/3219393/
stdlib-and-colored-output-in-c}
\end{verbatim}
% -- Bibliography (APA style)
%\bibliography{references}

\end{document}